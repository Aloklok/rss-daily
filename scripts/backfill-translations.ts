/**
 * å­˜é‡æ–‡ç« ç¿»è¯‘è„šæœ¬
 *
 * å°†æ‰€æœ‰æœªç¿»è¯‘çš„ä¸­æ–‡æ–‡ç« æ‰¹é‡ç¿»è¯‘ä¸ºè‹±æ–‡
 *
 * ä¸Šä¸‹æ–‡è®¡ç®—ï¼š
 * - Qwen3-8B ä¸Šä¸‹æ–‡çª—å£ï¼š128K tokens
 * - æ¯ç¯‡æ–‡ç« çº¦ 2500 tokensï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
 * - ç†è®ºä¸Šé™ï¼š~50 ç¯‡/æ‰¹æ¬¡
 * - ä¿å®ˆè®¾ç½®ï¼šé€ç¯‡ç¿»è¯‘ï¼Œé¿å…ä¸Šä¸‹æ–‡æº¢å‡ºå’Œ API é™é¢‘
 *
 * ä½¿ç”¨æ–¹æ³•ï¼š
 *   npx tsx scripts/backfill-translations.ts [--limit N] [--batch]
 *
 * å‚æ•°ï¼š
 *   --limit N    é™åˆ¶ç¿»è¯‘æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
 *   --batch      å¯ç”¨æ‰¹é‡æ¨¡å¼ï¼ˆé»˜è®¤ä¸ºé€ç¯‡ç¿»è¯‘æ¨¡å¼ï¼‰
 */
import * as dotenv from 'dotenv';
dotenv.config({ path: '.env.local' });

import { createClient } from '@supabase/supabase-js';
import { translateBatchAndSave } from '../src/domains/intelligence/services/translate';
import {
  DEFAULT_TRANSLATION_MODEL,
  HUNYUAN_TRANSLATION_MODEL,
} from '../src/domains/intelligence/constants';

const supabaseUrl = process.env.SUPABASE_URL!;
const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;
const supabase = createClient(supabaseUrl, supabaseKey);

// è§£æå‘½ä»¤è¡Œå‚æ•°
const limitArg = process.argv.find((arg) => arg.startsWith('--limit='));
const limit = limitArg ? parseInt(limitArg.split('=')[1], 10) : undefined;
const isBatch = process.argv.includes('--batch'); // é»˜è®¤ä¸º single æ¨¡å¼ï¼Œéœ€æ˜¾å¼ä¼  --batch å¼€å¯æ‰¹é‡

// é…ç½® (é»˜è®¤ä¸º single æ¨¡å¼ï¼šé€ç¯‡ç¿»è¯‘ï¼Œä½¿ç”¨æ··å…ƒæ¨¡å‹)
const BATCH_SIZE = isBatch ? 5 : 1;
const CONCURRENCY = isBatch ? 3 : 1;
const CURRENT_MODEL = isBatch ? DEFAULT_TRANSLATION_MODEL : HUNYUAN_TRANSLATION_MODEL;
const DELAY_BETWEEN_BATCHES_MS = isBatch ? 1000 : 500;

/**
 * é€’å½’è·å–æ‰€æœ‰ IDï¼Œçªç ´ Supabase 1000 æ¡é™åˆ¶
 */
async function fetchAllIds(tableName: string, hasSummary: boolean = false) {
  let allIds: { id: string | number }[] = [];
  let from = 0;
  const PAGE_SIZE = 1000;

  while (true) {
    let query = supabase
      .from(tableName)
      .select('id')
      .range(from, from + PAGE_SIZE - 1);

    if (hasSummary) {
      query = query.not('summary', 'is', null);
    }

    const { data, error } = await query;

    if (error) {
      console.error(`âŒ Failed to fetch IDs from ${tableName}:`, error);
      throw error;
    }

    if (!data || data.length === 0) break;

    allIds = allIds.concat(data);
    if (data.length < PAGE_SIZE) break;
    from += PAGE_SIZE;
  }

  return allIds;
}

async function backfillTranslations() {
  console.log('ğŸŒ Starting backfill translations (Concurrent Mode)...');
  console.log(`ğŸ¤– Model: ${CURRENT_MODEL}${!isBatch ? ' (Single Mode ğŸ¯)' : ' (Batch Mode ğŸ“¦)'}`);
  console.log(`ğŸ“¦ Batch Size: ${BATCH_SIZE} | âš¡ Concurrency: ${CONCURRENCY}`);

  if (limit) {
    console.log(`ğŸ“Š Limit: ${limit} articles`);
  }

  // 1. è·å–æ‰€æœ‰ ID ä»¥è¿›è¡Œç²¾å‡†å·®é›†è®¡ç®— (å…¨é‡æŠ“å–)
  let allArticleIdList: (string | number)[] = [];
  let translatedIdSet = new Set<string | number>();

  try {
    const [allIds, tIds] = await Promise.all([
      fetchAllIds('articles', true),
      fetchAllIds('articles_en'),
    ]);
    allArticleIdList = allIds.map((r) => r.id);
    translatedIdSet = new Set(tIds.map((r) => r.id));
  } catch (_e) {
    return;
  }

  const untranslatedIds = allArticleIdList.filter((id) => !translatedIdSet.has(id));
  const totalPending = untranslatedIds.length;

  console.log(`âœ… Already translated: ${translatedIdSet.size} articles`);
  console.log(`ğŸ“‰ Total pending: ${totalPending} articles`);

  if (totalPending === 0) {
    console.log('âœ¨ All articles processed!');
    return;
  }

  const activeLimit = limit || 100;
  const idsToProcess = untranslatedIds.slice(0, activeLimit);

  // 2. è·å–ç›®æ ‡æ–‡ç« å†…å®¹
  const { data: finalArticles, error: contentError } = await supabase
    .from('articles')
    .select(
      'id, title, category, summary, tldr, highlights, critiques, "marketTake", keywords, link, "sourceName", published, n8n_processing_date, verdict',
    )
    .in('id', idsToProcess)
    .order('published', { ascending: false });

  if (contentError) {
    console.error('âŒ Failed to fetch content:', contentError);
    return;
  }

  const total = finalArticles.length;
  console.log(`ğŸ“ Total articles in this run: ${total}`);

  // 3. å¹¶å‘æ‰¹é‡å¤„ç†
  let totalSuccess = 0;
  let totalFailed = 0;
  let completedCount = 0;

  // å°†æ–‡ç« åˆ‡åˆ†ä¸ºç­‰é¢æ‰¹æ¬¡
  const batches: any[][] = [];
  for (let i = 0; i < total; i += BATCH_SIZE) {
    batches.push(finalArticles.slice(i, i + BATCH_SIZE));
  }

  console.log(`ğŸš€ Processing ${batches.length} batches with concurrency of ${CONCURRENCY}...\n`);

  // å®šä¹‰å•ä¸ªä»»åŠ¡æ‰§è¡Œå™¨
  const processBatch = async (batch: any[], batchIndex: number) => {
    const chunk = batch.map((article) => ({
      id: String(article.id),
      title: article.title || '',
      category: article.category || '',
      summary: article.summary || '',
      tldr: article.tldr || '',
      highlights: article.highlights || '',
      critiques: article.critiques || '',
      marketTake: article.marketTake || '',
      keywords: Array.isArray(article.keywords) ? article.keywords : [],
      link: article.link,
      sourceName: article.sourceName,
      published: article.published,
      n8n_processing_date: article.n8n_processing_date,
      verdict: article.verdict,
    }));

    try {
      let result = await translateBatchAndSave(chunk, CURRENT_MODEL);

      // FALLBACK: If batch fails, try each article individually to isolate "bad" articles
      if (!result.success && chunk.length > 1) {
        console.warn(
          `âš ï¸ Batch #${batchIndex + 1} failed. Falling back to individual processing for ${chunk.length} articles...`,
        );
        let subSuccess = 0;
        for (const item of chunk) {
          const subResult = await translateBatchAndSave(
            [item],
            !isBatch ? HUNYUAN_TRANSLATION_MODEL : DEFAULT_TRANSLATION_MODEL,
          );
          if (subResult.success) {
            subSuccess += subResult.count;
          } else {
            console.error(`  âŒ Individual fallback failed for ${item.id}: ${subResult.error}`);
          }
        }
        result = {
          success: subSuccess > 0,
          count: subSuccess,
          error:
            subSuccess < chunk.length
              ? `Partial failure (${chunk.length - subSuccess} articles skipped)`
              : undefined,
        };
      }

      completedCount += chunk.length;
      const progress = ((completedCount / total) * 100).toFixed(1);

      if (result.success) {
        totalSuccess += result.count;
        console.log(
          `âœ… [${completedCount}/${total}] (${progress}%) Batch #${batchIndex + 1} Done: ${result.count}/${chunk.length} stored.`,
        );
        if (result.count < chunk.length) {
          totalFailed += chunk.length - result.count;
        }
      } else {
        totalFailed += chunk.length;
        console.error(
          `âŒ [${completedCount}/${total}] Batch #${batchIndex + 1} Failed: ${result.error}`,
        );
      }
    } catch (e: any) {
      completedCount += chunk.length;
      totalFailed += chunk.length;
      console.error(`ğŸ’¥ Batch #${batchIndex + 1} Fatal error: ${e.message}`);
    }
  };

  // ä½¿ç”¨ç®€å•çš„å¹¶è¡Œæ± é€»è¾‘
  for (let i = 0; i < batches.length; i += CONCURRENCY) {
    const pool = batches
      .slice(i, i + CONCURRENCY)
      .map((batch, idx) => processBatch(batch, i + idx));
    await Promise.all(pool);

    if (i + CONCURRENCY < batches.length) {
      await new Promise((resolve) => setTimeout(resolve, DELAY_BETWEEN_BATCHES_MS));
    }
  }

  // 4. è¾“å‡ºç»“æœ
  console.log('\n' + '='.repeat(60));
  console.log('ğŸ Batch backfill completed!');
  console.log(`âœ… Success (Total): ${totalSuccess}`);
  console.log(`âŒ Failed (Total): ${totalFailed}`);
  console.log(`ğŸ“‰ Still remaining overall: ${totalPending - totalSuccess} articles`);
  console.log('='.repeat(60));
}

backfillTranslations().catch(console.error);
