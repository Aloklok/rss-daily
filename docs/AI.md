# AI 智能体架构文档 (AI.md)

本项目集成了一套基于 **RAG (Retrieval-Augmented Generation)** 模式的高级 AI 对话系统，旨在为管理员提供基于本地简报和行业时事的智能问答能力。

## 🏗 核心架构 (RAG 2.0 流路)

系统的执行链路分为三个阶段：

### 0. 向量化 (Vectorization)

在进入 RAG 流程前，所有文章均通过 `scripts/backfill-embeddings.ts` 或 `gemini.ts` 进行向量化预处理。我们采用**混合语义指纹**策略，而非单纯的正文切片。

- **Embedding 模型**: `gemini-embedding-001`
- **向量化字段**: `title` + `category` + `keywords` + `summary` + `tldr`
- **逻辑**: 通过将分类和关键词硬编码进向量内容，确保了语义搜索时不仅能匹配到内容相似，还能匹配到“分类正确”的文章，大幅提升召回准确率。

### 1. 召回阶段 (Recall Phase)

为了平衡召回率与引用的精确度，系统采用两阶段过滤：

- **第一阶段（混合检索 Hybrid Search）**：
  - **PGroonga Search (Rank 1)**: 使用 PGroonga 扩展 (`&@~`) 进行多语言全文检索。
    - **优势**: 完美支持中文分词、英文单词及混合检索（如 "CentOS替代" -> 命中 "CentOS" 和 "替代"），解决了 PostgreSQL 原生 FTS 对中文支持不佳的问题。
    - **实现**: 采用拆分索引策略（文本字段 vs JSONB 字段）以绕过 PGroonga 对混合数据类型的限制。
  - **High Vector (Rank 2)**: 语义相似度 `> 0.80`。代表“强相关”或“同义词”匹配（即使没有关键词命中）。
  - **Mid Vector (Rank 3)**: 语义相似度 `> 0.65`。作为兜底召回，捕捉潜在的相关性。
- **第二阶段（精选/重排）**：
  - **输入**: 将 Top 50 篇的 `id, title, published, sourceName, summary, category, keywords` 组装为 Prompt。
  - **交互**: 调用 Gemini (2.0 Flash) 执行语义重排 (`reRankArticles`)。
  - **逻辑**: 模型基于 User Query 剔除内容重复或时效性差的篇目。
  - **结果**: 选出最相关的 **Top 10-15** 篇作为最终上下文。这极大降低了模型的“注意力发散”，提高了引用的准确度。

### 2. 增强上下文生成 (Context Enrichment)

在精选出 Top 15 后，系统会构建一个高度浓缩的背景列表供模型处理：

- **字段提取**: 从数据库中提取以下核心元数据：
  - **基础**: `id`, `title`, `published`, `link`, `sourceName`
  - **分类**: `category`, `keywords`
  - **摘要**: `tldr` (一句话总结), `summary`
  - **深度分析**: `verdict` (评分与重要性), `highlights` (技术亮点), `critiques` (犀利吐槽), `marketTake` (市场观点)
- **注入 Persona**: 注入“首席架构师” Persona，强制执行差异化引用逻辑（见下文）。

### 3. 流式渲染与交互 (Streaming & UX)

- **SSE 流式输出**：通过 Server-Sent Events 实现毫秒级响应，并配合缓冲区解决数据包截断问题。
- **交互渲染引擎**：前端通过深度递归解析 Markdown，将文本中的 `[N]` 自动转化为交互组件。点击后不再打开外部链接，而是直接触发 `UnifiedArticleModal` 并默认进入“简报”视图。对于未缓存的文章，系统会自动触发异步补全（Hydration）流程。

---

## 🔍 引用准则 (Citation Rules)

为了确保专业度，系统遵循以下“差异化引用”策略：

- **常识不引**：互联网通用知识、基础概念不添加脚注。
- **证据必引**：涉及具体数据、特定项目案例、独到见解时，强制在句末标注 `[N]`。
- **透明审计**：每条回复末尾均有 `[统计：检索 X 篇，引用了 Y 篇]`，方便管理员溯源。

---

## ⚙️ 模型与配额管理

- **模型映射层**：系统内置了名称自动降级/纠偏逻辑（例如 `gemini-3-flash` 自动降级为 `2.0-flash`），确保由于 Google API 更新导致的 ID 失效不会触发 404。
- **使用看板**：对话框型号选择器中实时显示模型配额（RPM/RPD），帮助管理员选择性价比最高的型号。

---

## ⚠️ 开发注意事项 (Important Notes)

### 1. 渲染性能与 VDOM 稳定性

- **递归解析**: 系统的引用按钮采用深度递归解析方案。虽然功能强大（支持嵌套在加粗/斜体内），但计算效率受对话长度影响。
- **对话锁定**: `ChatStore` 会根据记忆窗口（默认 8 轮）自动管理。如需更长历史，需在 `chatStore.ts` 中调整以平衡内存与性能。
- **组件持久化 (Critical)**:
  - **组件隔离**: `MessageList` 已被抽离为独立的 `React.memo` 组件，确保在流式输出（Partial Streaming）触发高频重绘时，只有最后一条消息在更新，上百条历史消息保持静态。这是防止多轮对话 CPU 飙升的关键。
  - **顶层声明**: `ChatMessageItem` 和 `MessageList` 必须声明在 **顶层作用域**（即模态框主组件外部）。严禁在 `AIChatModal` 内部动态声明组件，否则 React 每次 Render 都会认为其是新组件，强制卸载旧 DOM 并重新挂载，这将使 `React.memo` 完全失效并导致剧烈卡顿。

### 2. 模型配额与 API 隔离

- **物理隔离**: 为防止实时问答与后台定时任务（如大规模简报生成）争抢配额，AI 助手强制锁定使用 `GOOGLE_GENERATIVE_AI_API_KEY_CHENG30`。
- **模型路由**: 在 `app/api/ai/chat/route.ts` 中维护有一层名称映射逻辑。若 Google 发布新模型（如 Gemini 3.0），必须在此进行名称别名注册以防 404。

### 3. 样式保护与视觉架构

- **Prose 作用域**: 消息内容强制包裹在 `.prose` 类中。修改全局样式时，应优先使用 Tailwind 的 `prose-xxx` 工具类或通过 `globals.css` 中针对 AI 容器的定向选择器来调整，避免全局样式污染。
- **模糊度设计 (Blur Rationale)**: 模态框背景模糊度限制在 `backdrop-blur-xl`。实测表明，在流式打字高频刷新时，`2xl` 或更高强度的 GPU 高斯模糊计算会与文字渲染产生资源竞争，导致明显的低帧率感。

---

## 🚀 深度工程优化建议 (Optimization Suggestions)

基于当前版本的交付质量与用户反馈，以下是下一步建议的工程优化方向：

### 1. 架构组件化重构 (Code Modularization)

- **现状**: `AIChatModal.tsx` 已膨胀至 700+ 行，逻辑高度耦合。
- **建议**: 将其拆分为 `ChatHeader`、`MessageList`、`MessageItem`、`CitationPanel`、`ModelSelector` 和 `ChatInput`。
- **收益**: 提高组件的可维护性，并通过 React.memo 进一步精简重绘范围，提升流畅度。

### 2. 语义缓存层 (Semantic Caching)

- **建议**: 在 `app/api/ai/chat/route.ts` 中引入语义缓存。
- **逻辑**: 对用户的 Query 进行向量化，如果与 10 分钟内已回答过的 Query 相似度高于 0.98，则直接返回缓存结果。
- **收益**: 极大减少 Gemini 配额消耗，实现“秒回”体验。

### 3. 引用系统的极致鲁棒性 (Citation Robustness)

- **多格式兼容**: 采用统一的 `getOriginalIndex` 辅助函数，确保 `[1]`、`[ 1 ]`、`¹` 等各种 AI 可能输出的变体都能被精准捕获。
- **后置修正机制**: 在数据持久化前端增加“自动校准”逻辑，纠正 AI 自报的统计数字与实际提取到的文献数量之间的差异。
- **首次出现原则**: 实施“每个引用仅在正文首次出现时转为角标”的策略，减少视觉干扰，保持排版轻盈。

### 4. 样式与性能的平衡 (Style vs. Perf)

- **纯色优先**: 为了极致的流式打字性能，在全屏模式下应优先使用纯色背景（去除 `backdrop-blur`），避免 GPU 重绘与文字渲染争抢资源。
- **全屏自适应**: 模态框应支持“真全屏”切换（Edge-to-Edge），但在宽屏设备上必须通过 `max-w-5xl` 限制对话气泡宽度，保护阅读体验。

---

## 📅 路线图 (Roadmap)

1. **[待办] 混合搜索深度集成**：接入 Serper 或 Bing API，弥补 Google Search Grounding 在特定中文垂直领域的信息差。
2. **[进行中] 对话状态完全持久化**：确保元数据（Metadata）随消息体一同落库，彻底解决页面刷新后历史引用失效的问题。
3. **[计划] 智能搜索触发器**：根据本地检索文章的相关度评分，自动决定是否静默开启联网搜索。
4. **[计划] 多模态支持**：基于 Gemini 2.0 Pro 能力，支持管理员直接拖入截图让 AI 分析报表。

### 5. 高级检索策略 (Search Intelligence)

- **[已完成] 关键词搜索升级**: 已将 RPC 升级为 **PGroonga** (`&@~`)，实现对中英文混合内容的无缝全文检索。相比原生 FTS，它不需要额外维护分词词典，对专有名词支持更好。
- **[计划] 智能路由 (AI Router)**: 在 RAG 链路上游引入轻量级路由层 (Gemini 1.5 Flash)，负责：
  - **意图识别**: 区分“闲聊”、“普通搜索”和“结构化筛选”。
  - **参数提取**: 将自然语言（如“上周高分文章”）转化为 SQL `WHERE` 过滤条件，解决结构化搜索难题。
  - **上下文复用**: 智能判断是否需要跳过搜索直接复用上一轮 Context。

### 6. 异步任务架构 (Scalability)

- **[架构建议] 引入 PGMQ**: 当前系统采用同步生成 Embedding，适合单条数据流。若未来扩展至**批量抓取**场景（如一次导入 500 篇），强烈建议引入 `pgmq`（Supabase 扩展）：
  - **流程**: 爬虫入库 -> 写入 PGMQ 队列 -> Worker 消费队列 -> 调用 Gemini 生成向量 -> 异步回写 DB。
  - **收益**: 解耦 AI 处理的长耗时（5s+）与入库的短耗时（ms级），防止 API 超时。
